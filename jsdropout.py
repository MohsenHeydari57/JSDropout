# -*- coding: utf-8 -*-
"""JSDropout.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-M6MxMV3LB9BXR06O1rYAJf2RC7EM89K

# Dataset preparation
Mnist dataset is divided into two parts:
*   train (60000 X 28 X28)
*   test  (10000 X 28 X28)

In this example we are using a Fully Connected Neural Network so the data sample is reshaped from 28X28 to 784. The data is normalized between 0 and 1. to_categorical function. Converts a class vector (integers) to binary class matrix.
"""

from keras.datasets import mnist
import numpy as np
from tensorflow.keras.utils import to_categorical
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(60000, 784)
X_train = X_train.astype('float32')
X_train /= 255
Y_train = to_categorical(y_train)

X_test = X_test.reshape(10000, 784)
X_test = X_test.astype('float32')
X_test /= 255
Y_test = to_categorical(y_test)

from keras.datasets import mnist
import random
import numpy as np
from sklearn.ensemble import RandomForestRegressor
import math
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
from keras.optimizers import SGD
from keras.losses import categorical_crossentropy
from keras.callbacks import Callback
from tensorflow.keras.losses import MSE

"""# Preprocessing



*   X: X_train
*   y: Y_train
*   KBatch(X,y,K):  Divides the dataset into K baches.
*   RandomforestRank(X,y,k):  Ranks the features of each batch using random forest.
*   ConvertDistribute(RankList): Each vector converts the ranking into a statistical distribution.

*   JenssenShanon(DistributeVector): Calculates the Jensen-Shannon divergence of statistical distributions.

*  getInputWeight(X,y,k): The distribution with minimum divergence is chosen.


"""

def KBatch(X,y,K):
    index = []
    for i in range(0, len(y)):
        index.append(i)
    random.shuffle(index)
    Kx=[]
    Ky=[]
    step=round(len(y)/K)
    for i in range(0, K-1):
        inx=index[i*step:(i+1)*step]
        Kx.append(X[inx])
        Ky.append(y[inx])

    inx=index[(K-1)*step:len(y)]
    Kx.append(X[inx])
    Ky.append(y[inx])
    return(Kx,Ky)

def Rank(x):
    R=[];
    d=len(x)
    for i in range(0, d):
       r=1
       for j in range(0, d):
          if(x[i]<x[j]):
             r=r+1
       if findlist(R,r)>-1:
           r=r+1
       R.append(r)
    return(R)

def RandomforestRank(X,y,k):
    Kx,Ky = KBatch(X,y,k)
    print('Batching...',k)
    RankList=[]
    Weight=[]
    for i in range(0, k):
        rf = RandomForestRegressor()
        rf.fit(Kx[i], Ky[i])
        fi=rf.feature_importances_
        RankList.append(Rank(fi))
        Weight.append(fi)
        print('RandomForest...',i)
    return(RankList,Weight)


def ConvertDistribute(RankList):
  DistVector=[]
  d=len(RankList[0])
  ki=len(RankList)
  for k in range(0, ki):
    x=RankList[k]
    dv=[]
    for i in range(0, d):
      s=0
      for j in range(0, d-x[i]+1):
          s=s+1/(x[i]+j)
      s=(1+s)/(2*d)
      dv.append(s)
    DistVector.append(dv)
  return(DistVector)

def findlist(x,a):
    inx=-1
    for i in range(0, len(x)):
        if(x[i]==a):
           inx=i
    return(inx)


def JenssenShanon(DistributeVector):
  Dp=[]
  d=len(DistributeVector[0])
  k=len(DistributeVector)

  for i in range(0, d):
     p=0
     for j in range(0, k):
        p=p+DistributeVector[j][i]
     Dp.append(p/k)

  Djs=[]
  for i in range(0, k):
     s=0
     for j in range(0, d):
       p=DistributeVector[i][j]
       s=s+p*math.log(p/Dp[j])
     Djs.append(s/k)
  print('JenssenShanon...')
  return(Djs)


def getInputWeight(X,y,k):
  RankList,Weight = RandomforestRank(X,y,k)
  DistributeVector=ConvertDistribute(RankList)
  Djs=JenssenShanon(DistributeVector)
  i=np.argmin(Djs)
  return DistributeVector[i]

Z0=getInputWeight(X_train,y_train,10)

"""# JSDropout Class
JSDropout is customized based on keras.dropout source. The input to the JSDropout is the dropout **rate** and the number of the network layer on which the dropout is performed.

The **mask** in the call() is a boolean vector with the length of the number of nodes in the dropout layer, which specifies active and inactive nodes with true and false.

**getJSDmask():** based on the JSDropout method , determines the list of active and inactive nodes.


**ZWeight** is the weight calculated by forward propagation.
"""

class JSDropout(Layer):
    def __init__(self,  rate,Zinx, **kwargs):
        super(JSDropout, self).__init__(**kwargs)
        self.rate = rate
        self.Zinx = Zinx


    def call(self, inputs, training=None):
        if training:
           mask=self.getJSDmask()
           return tf.where(mask, inputs, tf.zeros_like(inputs))
        return inputs

    def getJSDmask(self):
           global ZWeight
           Nweight=ZWeight[self.Zinx].copy()
           Nweight=[sorted(Nweight).index(x) for x in Nweight]
           alpha=1.0/(len(Nweight)-1)
           Nweight=list(map(lambda x : x*alpha, Nweight))
           j=0
           Rate=len(Nweight)*self.rate
           for i in range(len(Nweight)):
               if(j<Rate and Nweight[i]>random.random()):
                 Nweight[i]=True
                 j+=1
               else:
                 Nweight[i]=False
           Nweight=np.array([Nweight]).tolist()
           mask=tf.convert_to_tensor(Nweight)
           return(mask)

"""# Forward propagation

*   forwardPropogate(): Calculating the Zweight using Z0 and forward propagation.
*   JSCallback(Callback): Using the callback technique, forwardPropagation() is performed during training and updates the Zweights


"""

class JSCallback(Callback):
    #def on_train_batch_begin(self, batch, logs=None):
        #forwardPropogate()
        #print("Start batch {} forwardpropagation".format(batch))
    def on_epoch_begin(self, epoch, logs=None):
        forwardPropogate()
        print("forwardpropagation {}".format(epoch))



def forwardPropogate():
    global ZWeight
    global Z0
    Weights=[]
    Weights.append(Model.get_layer("L0").get_weights()[0])
    Weights.append(Model.get_layer("L1").get_weights()[0])
    ZWeight=[]
    Z=np.array(Z0)
    for i in range(int(len(Weights))):
       Z=np.matmul(Z, Weights[i])
       Z=activations.relu(Z).numpy()
       ZWeight.append(Z)

"""# Deep Neural Network Architecture"""

Model = Sequential()
Model.add(Dense(200, activation='relu', input_shape=(784,),name="L0"))
Model.add(JSDropout(0.5,0))
Model.add(Dense(200, activation='relu',name="L1"))
Model.add(JSDropout(0.5,1))
Model.add(Dense(10, activation='softmax'))
Model.summary()
opt = Adam(learning_rate=0.001)
Model.compile(optimizer=opt, loss=MSE, metrics=['accuracy'])

"""# Train"""

history=Model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_split=0.2,callbacks=[JSCallback()])

"""# Test"""

train_results = Model.evaluate(X_train, Y_train, batch_size=128)
print("train loss, train acc:", train_results)
test_results = Model.evaluate(X_test, Y_test, batch_size=128)
print("test loss, test acc:", test_results)
gap=train_results[1]-test_results[1]
print("Gap:", gap)

