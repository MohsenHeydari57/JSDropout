# JSDropout: Forward propagation dropout in deep neural networks using Jensen–Shannon and random forest feature importance ranking
In this research project, using random forest and Jensen–Shannon divergence, the importance of each node is calculated once. Then, in the forward propagation steps, the importance of the nodes is propagated and used in the dropout mechanism. This method is evaluated and compared with some previously proposed dropout approaches using two different deep neural network architectures on the MNIST dataset. 
 
### Colab:
[JSDropout Colab link](https://colab.research.google.com/drive/1-M6MxMV3LB9BXR06O1rYAJf2RC7EM89K?usp=sharing)

### Reference:
Heidari, M., Moattar, M. H., & Ghaffari, H. (2023). [Forward propagation dropout in deep neural networks using Jensen-Shannon and random forest feature importance ranking.](https://doi.org/10.1016/j.neunet.2023.05.044) Neural Networks: The Official Journal of the International Neural Network Society, 165, 238–247. 

